title: Text generation Webui
name: textgen
use_python: true
python_version: 10
expose: true
port: 7009

prepare_repo: |-
  # Remove stale symlink to avoid pull conflicts
  rm -rf $LINK_MODEL_TO

  TARGET_REPO_DIR=$REPO_DIR \
  TARGET_REPO_BRANCH="main" \
  TARGET_REPO_URL="https://github.com/oobabooga/text-generation-webui" \
  UPDATE_REPO=${{ name|upper }}_UPDATE_REPO \
  UPDATE_REPO_COMMIT=${{ name|upper }}_UPDATE_REPO_COMMIT \
  prepare_repo

prepare_env: |-
  cd $REPO_DIR
  pip install torch torchvision torchaudio
  pip install -r requirements.txt

  mkdir -p repositories
  cd repositories
  TARGET_REPO_DIR=$REPO_DIR/repositories/GPTQ-for-LLaMa \
  TARGET_REPO_BRANCH="cuda" \
  TARGET_REPO_URL="https://github.com/qwopqwop200/GPTQ-for-LLaMa.git" \
  prepare_repo
  
  cd GPTQ-for-LLaMa
  python setup_cuda.py install

  pip uninstall -y llama-cpp-python
  CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

  pip install deepspeed

  pip install xformers

download_model: |-
  # Prepare model dir and link it under the models folder inside the repo
  mkdir -p $MODEL_DIR
  rm -rf $LINK_MODEL_TO
  ln -s $MODEL_DIR $LINK_MODEL_TO
  if [[ ! -f $MODEL_DIR/config.yaml ]]; then 
      current_dir_save=$(pwd) 
      cd $REPO_DIR
      commit=$(git rev-parse HEAD)
      wget -q https://raw.githubusercontent.com/oobabooga/text-generation-webui/$commit/models/config.yaml -P $MODEL_DIR
      cd $current_dir_save
  fi


  llm_model_download

action_before_start: |-
  if env | grep -q "PAPERSPACE"; then
    sed -i "s/server_port=shared.args.listen_port, inbrowser=shared.args.auto_launch, auth=auth)/server_port=shared.args.listen_port, inbrowser=shared.args.auto_launch, auth=auth, root_path='\\/textgen')/g" $REPO_DIR/server.py
  fi

start: |-
  cd $REPO_DIR
  share_args="--chat --listen-port ${{ name|upper }}_PORT --xformers ${EXTRA_{{ name|upper }}_ARGS}"
  if [ -v {{ name|upper }}_ENABLE_OPENAI_API ] && [ ! -z "${{ name|upper }}_ENABLE_OPENAI_API" ];then
    loader_arg=""
    if echo "${{ name|upper }}_OPENAI_MODEL" | grep -q "GPTQ"; then
      loader_arg="--loader exllama"
    fi
    if echo "${{ name|upper }}_OPENAI_MODEL" | grep -q "LongChat"; then
      loader_arg+=" --max_seq_len 8192 --compress_pos_emb 4"
    fi
    PYTHONUNBUFFERED=1 OPENEDAI_PORT=7013 service_loop "python server.py --model ${{ name|upper }}_OPENAI_MODEL $loader_arg --extensions openai $share_args" > $LOG_DIR/{{ name }}.log 2>&1 &
  else
    PYTHONUNBUFFERED=1 service_loop "python server.py  $share_args" > $LOG_DIR/{{ name }}.log 2>&1 &
  fi
  echo $! > /tmp/{{ name }}.pid
  
  # undo the change for git pull to work
  if env | grep -q "PAPERSPACE"; then
    sed -i "s/server_port=shared.args.listen_port, inbrowser=shared.args.auto_launch, auth=auth, root_path='\\/textgen')/server_port=shared.args.listen_port, inbrowser=shared.args.auto_launch, auth=auth)/g" $REPO_DIR/server.py
  fi

custom_start: ""
custom_reload: ""
custom_stop: ""

export_required_env: ""
other_commands: |-
  export GRADIO_ROOT_PATH=/textgen
  export MODEL_DIR=${{ '{' ~ name|upper }}_MODEL_DIR:-"$DATA_DIR/llm-models"}
  export REPO_DIR=${{ '{' ~ name|upper }}_REPO_DIR:-"$ROOT_REPO_DIR/text-generation-webui"}

  export {{ name|upper }}_PORT= {{ port }}
  export {{ name|upper }}_OPENAI_API_PORT=${{ '{' ~ name|upper }}_OPENAI_API_PORT:-7013}
  export EXPOSE_PORTS="$EXPOSE_PORTS:${{ name|upper }}_PORT:${{ name|upper }}_OPENAI_API_PORT"
  export PORT_MAPPING="$PORT_MAPPING:{{ name }}:{{ name }}_openai_api"
  export HUGGINGFACE_TOKEN=$HF_TOKEN

  export LINK_MODEL_TO=${{ '{' ~ name|upper }}_LINK_MODEL_TO:-"$REPO_DIR/models"}

nginx_extra: |-
  location ~* ^/textgen-openai/(.*)$ {
      rewrite ^/textgen-openai/(.*)$ /$1 break;
      proxy_pass http://localhost:7013;
  }
